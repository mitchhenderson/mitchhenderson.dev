---
title: "Making 1 rep max estimates more accurate and honest"
description: |
  Use more of the information you have.
author: Mitch Henderson
date: 2025-12-09
thumbnailImage: /img/nrl_finals_probs.png
draft: true
filters: [details]
format:
  html:
    code-fold: true
    code-tools: true
    html-table-processing: none
knitr:
    opts_chunk:
      dev: "ragg_png"
---

### **tl;dr**: You can get more accurate 1RM estimates by using a statistical model that accounts for differences between athletes. They can also tell you how certain they are in their estimate (which matters).

<br>

[S&C coaches](https://en.wikipedia.org/wiki/Strength_and_conditioning_coach) collect training data to monitor progress and evaluate the effectiveness of their programming. A common method in strength training is to calculate an [**estimated one repetition maximum**](https://en.wikipedia.org/wiki/One-repetition_maximum#Estimating_1RM) (or e1RM) using a formula and track how it changes over time. Lots of coaches are happy to use e1RM as a progress indicator instead of testing a true 1RM during a program or in-season because they don't impact training (integrates into existing training sessions), are safer (lower weight), and data can be collected more frequently (faster program refinements). [True 1RM](https://en.wikipedia.org/wiki/One-repetition_maximum#Measuring_1RM) testing might only be programmed a handful of times per season (if at all) so using training data to regularly estimate strength increases or detect when a program should be adjusted is a good idea.

```{r}
#| output: false

library(tidyverse)
library(gganimate)
library(scales)
library(ggtext)
library(gt)
library(gtExtras)
library(sn) # skewed normal distribution function
library(brms)
library(tidybayes)
library(ggdist)

mitchhenderson::font_hoist("Myriad Pro")
socials <- mitchhenderson::social_caption(icon_colour = "dodgerblue")

colours <- c("#E31937", "#134A8E") # Come back to this

theme_set(
  theme_classic(base_size = 16, base_family = "Myriad Pro Regular") +
    theme(
      plot.title = element_text(face = "bold"),
      plot.subtitle = element_text(size = 14, color = "grey40", face = "bold"),
      axis.title.x = element_text(size = 14, color = "grey40", face = "bold"),
      plot.caption = element_markdown(color = "grey50", size = 10), # Maybe make bigger?
      axis.ticks = element_blank(),
      axis.line = element_line(colour = "grey50"),
      plot.title.position = "plot"
    )
)
```

## The problem

There are [lots of different formulas out there to estimate a 1RM](https://maxcalculator.com/guides/1rm-formulas).

```{r}
#| warning: false

# Define the 1RM estimation formulas
# Each formula calculates %1RM based on number of reps
formulas <- list(
  Epley = function(reps) {
    # Epley: 1RM = weight × (1 + reps/30)
    # Therefore: %1RM = 100 / (1 + reps/30)
    100 / (1 + reps / 30)
  },

  Brzycki = function(reps) {
    # Brzycki: 1RM = weight × (36/(37 - reps))
    # Therefore: %1RM = 100 × (37 - reps) / 36
    100 * (37 - reps) / 36
  },

  Mayhew = function(reps) {
    # Mayhew: 1RM = (100 × weight) / (52.2 + 41.9 × exp(-0.055 × reps))
    # Therefore: %1RM = 52.2 + 41.9 × exp(-0.055 × reps)
    52.2 + 41.9 * exp(-0.055 * reps)
  },

  Lombardi = function(reps) {
    # Lombardi: 1RM = weight × reps^0.10
    # Therefore: %1RM = 100 / reps^0.10
    100 / (reps^0.10)
  },

  `O'Conner` = function(reps) {
    # O'Conner: 1RM = weight × (1 + reps/40)
    # Therefore: %1RM = 100 / (1 + reps/40)
    100 / (1 + reps / 40)
  },

  Wathan = function(reps) {
    # Wathan: 1RM = (100 × weight) / (48.8 + 53.8 × exp(-0.075 × reps))
    # Therefore: %1RM = 48.8 + 53.8 × exp(-0.075 × reps)
    48.8 + 53.8 * exp(-0.075 * reps)
  },

  Lander = function(reps) {
    # Lander: 1RM = (100 × weight) / (101.3 - 2.67123 × reps)
    # Therefore: %1RM = 101.3 - 2.67123 × reps
    101.3 - 2.67123 * reps
  }
)

# Generate data for reps 1 to 20
reps_range <- 1:20

# Create a tidy dataframe with all formulas
e1rm_data <- tibble(reps = reps_range) |>
  crossing(formula = names(formulas)) |>
  mutate(
    percent_1rm = map2_dbl(formula, reps, ~ formulas[[.x]](.y))
  )

e1rm_data_animated <- e1rm_data |>
  crossing(highlight = unique(e1rm_data$formula)) |>
  mutate(
    is_highlighted = formula == highlight,
    line_color = if_else(is_highlighted, "highlighted", "grey"),
    label = if_else(is_highlighted & reps == max(reps), formula, NA_character_),
    line_size = if_else(is_highlighted, 2, 0.5)
  )

animated_plot <- e1rm_data_animated |>
  ggplot(aes(x = reps, y = percent_1rm, group = formula)) +
  coord_cartesian(xlim = c(1, 20), clip = "off") +
  geom_line(
    data = e1rm_data_animated |> filter(!is_highlighted),
    aes(linewidth = line_size),
    colour = "grey70"
  ) +
  geom_line(
    data = e1rm_data_animated |> filter(is_highlighted),
    aes(linewidth = line_size),
    colour = "#E31937"
  ) +
  geom_text(
    aes(label = label),
    colour = "#E31937",
    family = "Myriad Pro Regular",
    hjust = 0,
    nudge_x = 0.5,
    size = 6,
    fontface = "bold",
    show.legend = FALSE
  ) +
  scale_linewidth_identity() +
  scale_x_continuous(
    breaks = seq(1, 20, 1)
  ) +
  scale_y_continuous(
    labels = label_percent(scale = 1),
    breaks = seq(40, 100, 10),
    limits = c(40, 100)
  ) +
  labs(
    title = "1RM Estimation Formulas",
    x = "Number of Repetitions",
    y = NULL,
    subtitle = "% of 1RM",
    caption = socials
  ) +
  theme(
    # panel.grid = element_blank(),
    plot.margin = margin(10, 100, 10, 10),
    legend.position = "none"
  ) +
  transition_manual(
    highlight,
    cumulative = FALSE
  )

# Render the animation
animate(
  animated_plot,
  nframes = 80,
  fps = 10,
  width = 8,
  height = 4.5,
  device = "ragg_png"
)
```

But none of these:

-   Account for strength endurance differences between athletes

    Some athletes (fast twitch, higher metabolic cost per rep) have high 1RMs but find higher rep sets challenging. Other athletes (better oxidative capacity, lower metabolic cost per rep) can perform many reps at moderate intensities but quickly hit their 1RM ceiling with more weight. **I want my 1RM estimate to take into account what I know about the physiological characteristics of the athlete**. This directly impacts how accurate the e1RM will be. If a fast twitch athlete does a max reps set of 14 x 100kg, I expect their 1RM to be higher than a more oxidative athlete performing the same 14 x 100kg because I know they are better suited to high intensity work.

    *The standard formulas don't do this*.

-   Tell me how certain they are in their estimate

    The standard formulas WILL ALWAYS give you a 1RM estimate, doesn't matter how *UN*confident they are. The 1RM estimate will also always just be a single value. There's a big difference in how I'd interpret a 100kg e1RM if it's 80% certain of being between 98–102kg (pretty precise) versus 80% certain of being between 77.5–122.5kg (practically useless). There's also a big difference in 1RM certainty when the estimate is based on an athlete doing a max set of 2-5 reps (closer to true 1RM; higher certainty) compared 10+ reps (further from true 1RM; lower certainty). We also become more familiar about an athlete's ability the more we work with them and get to know them. I have a lot more confidence predicting the 1RM of an athlete I've worked with, gotten to know, and collected training data on for years compared to someone new with little training data available. **I want to incorporate all these sources of uncertainty into my 1RM estimate make the most informed decision**.

    *The standard formulas don't do this*.

## My solution

The idea is to extend one of the standard e1RM formulas by building it into a statistical model that takes into account which athlete it's estimating the 1RM for. It'll be fit using the athlete's past data so it learns how they handle higher weight vs higher reps. It's like a smarter version of the standard formula tailored to the athlete. Having this extra information helps it can make more accurate predictions.

The model will also be a [*Bayesian*](https://en.wikipedia.org/wiki/Bayesian_statistics) model. These have advantages over [conventional](https://en.wikipedia.org/wiki/Frequentist_probability) statistical models when it comes to expressing uncertainty ([among other things](https://www.youtube.com/watch?v=R6d-AbkhBQ8)). So instead of just getting a number, you get a full distribution representing the uncertainty in the 1RM estimate (e.g., "The 1RM estimate is 100 with 80% probability of it being between 98 and 102").[^1] This distribution accounts for all the sources of upstream uncertainty (amount of data available on the athlete lifting, number of reps they completed, etc).

[^1]: Note that this is different, and IMO much more intuitive, to what a confidence interval tells you. Confidence intervals are confusing [and even trained scientists often misinterpret them](https://pubmed.ncbi.nlm.nih.gov/24420726/).

To show you what I mean, we need some data.

### Simulate data

```{r}
set.seed(2534)

# Population hyperparameters (ground truth)
n_athletes <- 30
mean_n_obs_per_athlete <- 15
min_n_obs <- 5
max_n_obs <- 30

# True 1RM population parameters
pop_mean_1rm <- 140
pop_sd_1rm <- 15

# Endurance coefficient population parameters
pop_mean_endurance <- 30
pop_sd_endurance <- 4
# Maybe discuss importance of this on results (heterogenous endurance profiles = bigger gain from player specific parameters)

# Observation noise
obs_noise_sd <- 2.5

# Generate athlete-level ground truth
athletes <- tibble(
  athlete_id = 1:n_athletes,
  true_1rm = rnorm(n_athletes, mean = pop_mean_1rm, sd = pop_sd_1rm),
  true_endurance = rnorm(
    n_athletes,
    mean = pop_mean_endurance,
    sd = pop_sd_endurance
  ),
  n_observations = sample(min_n_obs:max_n_obs, n_athletes, replace = TRUE)
)

# Generate observations
simulated_data <- athletes |>
  rowwise() |>
  reframe(
    athlete_id = athlete_id,
    true_1rm = true_1rm,
    true_endurance = true_endurance,
    session_id = seq_len(n_observations),
    reps = sample(2:15, n_observations, replace = TRUE)
  ) |>
  mutate(
    # Theoretical weight from modified Epley formula
    weight_theoretical = true_1rm / (1 + reps / true_endurance),

    # Add normally distributed noise
    weight_observed = weight_theoretical + rnorm(n(), 0, obs_noise_sd)
  )
```

I've simulated 30 fake athletes, each with their own 1RM, strength endurance capacity, and number of previous sessions of data where they've performed a max reps set for this given exercise.

::: callout-important
I'm going light on simulation details here to keep this accessible. Expand the section below to get more statistical details on the parameters of the simulated data.
:::

::: {.details summary="WARNING: NERDS ONLY"}
I've defined the "squad" (30 athletes) to have a mean 1RM of 140 and standard deviation of 20 drawn from a random normal distribution. I also assign them an **endurance ability** (also from normal distribution).

Each athlete will have data from somewhere between 5 and 30 previous training sessions where they've done this exercises til failure.

For each of these max rep sets, the athletes complete somewhere between 2 and 15 reps (uniformly randomly sampled with replacement). I calculate a theoretical weight they should be able to complete for the set given their simulated 1RM and reps performed based on a modified Epley formula (probably most commonly used e1RM formula, modified to account for variable endurance ability).

I finally simulate the actual weight they lifted for the set by taking the weight they theoretically could lift given their simulated 1RM and reps, added some random noise (because humans work like that) from a random normal distribution (mean of 0, SD of 2.5).
:::

10 simulated athletes picked at random look like this:

```{r}
athletes |>
  slice_sample(n = 10) |>
  mutate(across(where(is.numeric), \(x) round(x, 1))) |>
  gt() |>
  gt_theme_538(quiet = TRUE)
```

I use these athletes and their characteristics to simulate realistic weights and reps. This gives me `r nrow(simulated_data)` simulated max rep sets across the 30 athletes.

10 picked at random look like this:

```{r}
simulated_data |>
  select(-starts_with("true"), -weight_theoretical) |>
  slice_sample(n = 10) |>
  mutate(across(where(is.numeric), \(x) round(x, 1))) |>
  gt() |>
  gt_theme_538(quiet = TRUE)
```

### The model

I'll specify the model to follow a modified [Epley formula](https://1repmaxcalculator.net/topics/epley-formula).

Reasons for this:

1.  It's probably the most commonly used of the standard 1RM estimate formulas

2.  It's fairly simple (good for demo purposes)

3.  I used it as part of how the data was simulated, so seeing how the model's performance compares to the standard Epley formula for predicting the true (albeit simulated) 1RMs will be a good test.

If I had real training data (instead of simulated), I would fit a bunch of different model specifications to see which one performs best.

The standard Epley is:

```{r}
#| eval: false
#| code-fold: false
weight * (1 + reps / 30)
```

The modification I'm using just changes the `30` (fixed constant representing how quickly strength drops off with more reps; same for everyone) to a parameter the model will optimise based on the data for each athlete. Each athlete gets their own value for this based on how they handle high intensity sets or high volume sets.

I'll fit the model on all the simulated data *except the most recent set for each athlete*. I'll keep these sets as a small holdout or test data for comparing predictive performance between my model and the standard Epley formula.

::: {.details summary="Nerd stats details about the model in here"}
Flesh these out: log transform k formula rearrangement partial pooling weakly informative priors prior predictive checks model diagnostics posterior predictive checks
:::

```{r}
#| output: false

# Data prep
model_data <- simulated_data |>
  rename(
    # brms doesn't like underscores or . in variable names
    athlete = athlete_id,
    orm = true_1rm,
    weight = weight_observed
  )

# Train/test split: hold out last observation per athlete
holdout <- model_data |>
  group_by(athlete) |>
  slice_max(session_id, n = 1) |>
  ungroup()

train <- model_data |>
  anti_join(holdout, by = c("athlete", "session_id"))

# Model specification
modified_epley_formula <- bf(
  weight ~ orm / (1 + reps / exp(logk)),
  orm ~ 1 + (1 | athlete),
  logk ~ 1 + (1 | athlete),
  nl = TRUE
)

# Prior specification
modified_epley_priors <- c(
  prior(normal(140, 30), nlpar = "orm", coef = "Intercept"),
  prior(normal(3.4, 0.3), nlpar = "logk", coef = "Intercept"), # log(30) ≈ 3.4
  prior(exponential(0.05), class = "sd", nlpar = "orm"),
  prior(exponential(2), class = "sd", nlpar = "logk"),
  prior(exponential(0.2), class = "sigma")
)


modified_epley_model_fit <- brm(
  formula = modified_epley_formula,
  data = train,
  prior = modified_epley_priors,
  family = gaussian(),
  cores = 4,
  chains = 4,
  iter = 4000,
  warmup = 2000,
  control = list(adapt_delta = 0.95, max_treedepth = 12),
  seed = 2534,
  file = "modified_epley_model_fit"
)

estimate_1rm <- function(model, athlete_id, weight, reps) {
  model |>
    spread_draws(b_logk_Intercept, r_athlete__logk[athlete, ]) |>
    filter(athlete == athlete_id) |>
    mutate(
      est_endurance = exp(b_logk_Intercept + r_athlete__logk),
      est_1RM = weight * (1 + reps / est_endurance)
    ) |>
    select(.draw, athlete, est_endurance, est_1RM)
}
```

Now the model is fit, I use it to make 1RM estimates with the help of a custom function I've defined called `estimate_1rm()`. It takes the model object, the athlete ID, and the weight and reps the athlete performed in the set as arguments and gives me back a distribution of predictions.

```{r}
#| code-fold: false
example_predictions <- estimate_1rm(
  model = modified_epley_model_fit,
  athlete_id = 17,
  weight = 102.5,
  reps = 5
)
```

```{r}
example_predictions |>
  ggplot(aes(x = est_1RM)) +
  stat_slab(fill = "skyblue") +
  stat_spike(at = "median", size = 0, colour = colours[2]) +
  scale_thickness_shared() +
  annotate(
    "curve",
    x = 120.5,
    xend = median(example_predictions$est_1RM) + 0.1,
    y = 1,
    yend = 0.925,
    curvature = 0.25,
    arrow = arrow(length = unit(2, "mm"), type = "closed"),
    colour = "grey60"
  ) +
  annotate(
    "text",
    x = 120.8,
    y = 0.95,
    label = "Median\n(summarised 1RM estimate)",
    hjust = 0.5,
    colour = "grey35",
    family = "Myriad Pro Regular"
  ) +
  annotate(
    "text",
    x = 116.5,
    y = 0.15,
    label = "Might be this low",
    hjust = 0.5,
    colour = "grey35",
    family = "Myriad Pro Regular"
  ) +
  annotate(
    "text",
    x = 120.9,
    y = 0.15,
    label = "Might be this high",
    hjust = 0.5,
    colour = "grey35",
    family = "Myriad Pro Regular"
  ) +
  labs(
    x = "Estimated 1RM",
    caption = socials,
    title = "Full distribution of 1RM estimates",
    subtitle = "Athlete 17 | Max reps set of 5 @ 102.5"
  ) +
  scale_x_continuous(breaks = seq(116, 122, 1)) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +
  theme(
    axis.text.y = element_blank(),
    axis.title.y = element_blank(),
    axis.line.y = element_blank()
  )
```

Sometimes the full distribution is a bit much and you just want a summary (e.g. median with 80% credible intervals).[^2] Easy.

[^2]: These intervals are what you get with Bayesian model estimates and they are what I think most people want confidence intervals to be. There is x% probability that the interval contains the true value.

```{r}
example_predictions |>
  median_qi(.width = 0.8) |>
  select(-contains("endurance"), -.interval) |>
  mutate(across(where(is.numeric), \(x) round(x, 1))) |>
  gt() |>
  gt_theme_538(quiet = TRUE)
```

## How accurate is it?

By seeing how close the model got to the true 1RM (from the simulation) when predicting from data it wasn't trained on, and doing the same for the standard Epley formula, we can compare their predictive accuracy for new data.[^3]

[^3]: This evaluation is pretty crude (hold out each athletes last set for testing; only 30 sets used for this). It's kind of how it would work in real life though; predictions after each new session are like their own test set (assuming model is updated before each session). A stronger approach during the research phase of implementing some thing like (as opposed to ongoing use) would be to do something like [cross validation](https://www.youtube.com/watch?v=fSytzGwwBVw).

```{r}
holdout_predictions <- holdout |>
  rowwise() |>
  mutate(
    model_prediction = estimate_1rm(
      modified_epley_model_fit,
      athlete,
      weight,
      reps
    ) |>
      summarise(prediction = mean(est_1RM)) |>
      pull(prediction),
    standard_epley_prediction = weight * (1 + reps / 30)
  ) |>
  ungroup() |>
  left_join(
    athletes |> select(athlete = athlete_id, true_1rm),
    by = "athlete"
  ) |>
  mutate(
    model_error = model_prediction - true_1rm,
    standard_epley_error = standard_epley_prediction - true_1rm,
    abs_difference = abs(model_error) - abs(standard_epley_error)
  )

# I'm thinking table where each row shows which method performed better (model wins by 2kg, coloured pill?)
```
